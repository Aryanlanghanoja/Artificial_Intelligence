# -*- coding: utf-8 -*-
"""Reinforcement_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AfZrhumr5Ttoq0Wmj_1YKLqKAr5C-6m4
"""

import numpy as np
import networkx as nx
import pylab as pl
import random

edges = [(0,1),(1,2),(1,3),(1,5),(5,6),(5,4),(9,10),(2,4),(0,6),(6,7),(8,9),(7,8),(1,7),(3,9),(10,8) , (0, 1), (1, 2), (2, 3), (3, 4), (4, 5),
         (5, 6), (6, 7), (7, 8), (8, 9), (9, 0),
         (1, 5), (6, 4), (3, 4)]

G = nx.Graph()
G.add_edges_from(edges)
pos = nx.spring_layout(G)
nx.draw_networkx_nodes(G,pos)
nx.draw_networkx_edges(G,pos)
nx.draw_networkx_labels(G,pos)
pl.show()

Matrix_size = 11
goal = 10

# Reward Matrix

R = np.matrix(np.ones((Matrix_size, Matrix_size)))
R *= -1
R

for edge in edges :
  if edge[1] == goal :
    R[edge] = 100
  else :
    R[edge] = 0

  if edge[0] == goal :
    R[edge[::-1]] = 100

  else :
    R[edge[::-1]] = 0

R[goal,goal] = 100

R

Q = np.matrix(np.zeros((Matrix_size, Matrix_size)))

initial_start = random.randint(0, 10)
initial_start

def available_state(state) :
  Currrent_state_row = R[state,]
  available_options = np.where(Currrent_state_row >-1)[1]
  return available_options

def next_state_selection(availabe_options) :
  return (int(np.random.choice(availabe_options , 1)))

def Q_update_value(current_state , action , gamma) :
  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]

  if(max_index.shape[0] > 1) :
    max_index = int(np.random.choice(max_index , size = 1))

  else :
    max_index = int(max_index)

  max_value = Q[action,max_index]

  Q[current_state,action] = R[current_state,action] + gamma * max_value

  if(np.max(Q)> 0) :
    # Corrected the line causing the error. np.max(Q) is used instead of np.
    return np.sum(Q / np.max(Q)*100)

  else :
    return 0

score=[]
for i in range(1000):
  current_state = np.random.randint(0,int(Q.shape[0]))
  available_act = available_state(current_state)
  action = next_state_selection(available_act)
  Q_update_value(current_state,action,gamma=0.8)
  score.append(Q_update_value(current_state,action,gamma=0.8))

pl.plot(score)

current_State = 2
steps = [current_State]

while(current_State != goal) :
  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]

  if(max_index.shape[0] > 1) :
    max_index = int(np.random.choice(max_index , size = 1))

  else :
    max_index = int(max_index)

  steps.append(max_index)
  current_State = max_index

steps

